{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e15177e",
   "metadata": {},
   "source": [
    "## Section 1: Environment & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756131de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/Users/ajaiupadhyaya/Documents/Models')\n",
    "\n",
    "from models.ml import RLReadyEnvironment\n",
    "from core.backtesting import SimpleMLPredictor\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Core libraries loaded.\")\n",
    "print(\"\\nNote: To use RL agents, install stable-baselines3:\")\n",
    "print(\"  pip install stable-baselines3\")\n",
    "\n",
    "# Try to import RL libraries\n",
    "try:\n",
    "    from stable_baselines3 import DQN, PPO, A2C\n",
    "    from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "    print(\"\\n✓ stable-baselines3 is installed!\")\n",
    "    RL_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"\\n⚠ stable-baselines3 not available (install for full RL examples)\")\n",
    "    RL_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4f0d49",
   "metadata": {},
   "source": [
    "## Section 2: Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed3e57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download market data\n",
    "print(\"Downloading market data...\")\n",
    "ticker = 'SPY'\n",
    "df = yf.download(ticker, period='2y', progress=False)\n",
    "\n",
    "# Split into train/val/test\n",
    "n = len(df)\n",
    "train_end = int(n * 0.6)\n",
    "val_end = int(n * 0.8)\n",
    "\n",
    "df_train = df.iloc[:train_end]\n",
    "df_val = df.iloc[train_end:val_end]\n",
    "df_test = df.iloc[val_end:]\n",
    "\n",
    "print(f\"Training data: {len(df_train)} days ({df_train.index[0].date()} to {df_train.index[-1].date()})\")\n",
    "print(f\"Validation data: {len(df_val)} days\")\n",
    "print(f\"Test data: {len(df_test)} days\")\n",
    "print(f\"\\nTotal price change: {(df['Close'].iloc[-1]/df['Close'].iloc[0]-1)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abd4e00",
   "metadata": {},
   "source": [
    "## Section 3: Create RL Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5da6d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training environment\n",
    "print(\"Creating RL trading environment...\")\n",
    "train_env = RLReadyEnvironment(df_train, initial_capital=100000)\n",
    "\n",
    "print(f\"\\nEnvironment Details:\")\n",
    "print(f\"  State shape: {train_env.observation_space.shape}\")\n",
    "print(f\"  Action space: {train_env.action_space.n} actions\")\n",
    "print(f\"  Action meanings: 0=hold, 1=long, 2=short, 3=close\")\n",
    "\n",
    "# Test environment\n",
    "print(f\"\\nTesting environment...\")\n",
    "state = train_env.reset()\n",
    "print(f\"  Initial state shape: {state.shape}\")\n",
    "print(f\"  Initial state (first 5 values): {state[:5]}\")\n",
    "\n",
    "# Take one step\n",
    "state, reward, done, info = train_env.step(1)  # action 1 = long\n",
    "print(f\"  After 1 step: reward={reward:.6f}, done={done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b9b896",
   "metadata": {},
   "source": [
    "## Section 4: Random Agent Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ed0956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random agent for comparison\n",
    "print(\"Running random agent baseline...\\n\")\n",
    "\n",
    "def run_agent(env, agent_func, episodes=1):\n",
    "    \"\"\"Run agent and return performance metrics.\"\"\"\n",
    "    returns = []\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while True:\n",
    "            action = agent_func(state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        returns.append(total_reward)\n",
    "    \n",
    "    return {\n",
    "        'returns': returns,\n",
    "        'mean_return': np.mean(returns),\n",
    "        'std_return': np.std(returns),\n",
    "        'final_performance': env.get_performance()\n",
    "    }\n",
    "\n",
    "# Random agent\n",
    "def random_agent(state):\n",
    "    return np.random.choice([0, 1, 2, 3])\n",
    "\n",
    "random_results = run_agent(train_env, random_agent, episodes=1)\n",
    "random_perf = random_results['final_performance']\n",
    "\n",
    "print(f\"Random Agent Performance:\")\n",
    "print(f\"  Mean Episode Return: {random_results['mean_return']:.2f}\")\n",
    "print(f\"  Final Capital: ${random_perf['final_capital']:,.2f}\")\n",
    "print(f\"  Total Return: {random_perf['total_return']*100:.2f}%\")\n",
    "print(f\"  Trades Made: {random_perf['trades']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaf38b2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de850d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smart agent using technical indicators\n",
    "print(\"Running smart baseline (technical indicators)...\\n\")\n",
    "\n",
    "predictor = SimpleMLPredictor(lookback_window=20)\n",
    "signals = predictor.predict(df_train)\n",
    "\n",
    "def smart_agent(state):\n",
    "    \"\"\"Agent that uses technical signals.\"\"\"\n",
    "    # Extract signal component from state (last element is often position info)\n",
    "    # For simplicity, use momentum-based logic\n",
    "    \n",
    "    # In real implementation, you'd access the internal signal\n",
    "    # Here we use a simple heuristic\n",
    "    if state[-2] > 0.5:  # High momentum\n",
    "        return 1  # Long\n",
    "    elif state[-2] < -0.5:  # Negative momentum\n",
    "        return 2  # Short\n",
    "    else:\n",
    "        return 0  # Hold\n",
    "\n",
    "smart_results = run_agent(train_env, smart_agent, episodes=1)\n",
    "smart_perf = smart_results['final_performance']\n",
    "\n",
    "print(f\"Smart Baseline Performance:\")\n",
    "print(f\"  Final Capital: ${smart_perf['final_capital']:,.2f}\")\n",
    "print(f\"  Total Return: {smart_perf['total_return']*100:.2f}%\")\n",
    "print(f\"  Trades Made: {smart_perf['trades']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f60b3d",
   "metadata": {},
   "source": [
    "## Section 6: DQN Agent Training (If stable-baselines3 available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072cbb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RL_AVAILABLE:\n",
    "    print(\"Training DQN agent...\\n\")\n",
    "    \n",
    "    # Create vectorized environment\n",
    "    train_env_vec = DummyVecEnv([lambda: RLReadyEnvironment(df_train, initial_capital=100000)])\n",
    "    \n",
    "    # Create DQN agent\n",
    "    dqn_agent = DQN(\n",
    "        'MlpPolicy',\n",
    "        train_env_vec,\n",
    "        learning_rate=0.001,\n",
    "        buffer_size=10000,\n",
    "        learning_starts=500,\n",
    "        exploration_fraction=0.1,\n",
    "        exploration_initial_eps=1.0,\n",
    "        exploration_final_eps=0.01,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Train for 10,000 steps\n",
    "    print(\"Training DQN for 10,000 steps...\")\n",
    "    dqn_agent.learn(total_timesteps=10000)\n",
    "    \n",
    "    print(\"Training complete!\\n\")\n",
    "    \n",
    "    # Evaluate DQN\n",
    "    print(\"Evaluating DQN on training data...\")\n",
    "    eval_env = RLReadyEnvironment(df_train, initial_capital=100000)\n",
    "    \n",
    "    state = eval_env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    while True:\n",
    "        action, _ = dqn_agent.predict(state, deterministic=True)\n",
    "        state, reward, done, info = eval_env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    dqn_perf = eval_env.get_performance()\n",
    "    print(f\"\\nDQN Performance:\")\n",
    "    print(f\"  Final Capital: ${dqn_perf['final_capital']:,.2f}\")\n",
    "    print(f\"  Total Return: {dqn_perf['total_return']*100:.2f}%\")\n",
    "    print(f\"  Trades Made: {dqn_perf['trades']}\")\n",
    "\n",
    "else:\n",
    "    print(\"DQN training requires stable-baselines3 installation.\")\n",
    "    print(\"Install with: pip install stable-baselines3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac4c3ee",
   "metadata": {},
   "source": [
    "## Section 7: PPO Agent Training (If stable-baselines3 available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b9f4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RL_AVAILABLE:\n",
    "    print(\"Training PPO agent...\\n\")\n",
    "    \n",
    "    # Create vectorized environment\n",
    "    train_env_vec = DummyVecEnv([lambda: RLReadyEnvironment(df_train, initial_capital=100000)])\n",
    "    \n",
    "    # Create PPO agent\n",
    "    ppo_agent = PPO(\n",
    "        'MlpPolicy',\n",
    "        train_env_vec,\n",
    "        learning_rate=0.0003,\n",
    "        n_steps=128,\n",
    "        batch_size=32,\n",
    "        n_epochs=10,\n",
    "        gae_lambda=0.95,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Train for 10,000 steps\n",
    "    print(\"Training PPO for 10,000 steps...\")\n",
    "    ppo_agent.learn(total_timesteps=10000)\n",
    "    \n",
    "    print(\"Training complete!\\n\")\n",
    "    \n",
    "    # Evaluate PPO\n",
    "    print(\"Evaluating PPO on training data...\")\n",
    "    eval_env = RLReadyEnvironment(df_train, initial_capital=100000)\n",
    "    \n",
    "    state = eval_env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    while True:\n",
    "        action, _ = ppo_agent.predict(state, deterministic=True)\n",
    "        state, reward, done, info = eval_env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    ppo_perf = eval_env.get_performance()\n",
    "    print(f\"\\nPPO Performance:\")\n",
    "    print(f\"  Final Capital: ${ppo_perf['final_capital']:,.2f}\")\n",
    "    print(f\"  Total Return: {ppo_perf['total_return']*100:.2f}%\")\n",
    "    print(f\"  Trades Made: {ppo_perf['trades']}\")\n",
    "\n",
    "else:\n",
    "    print(\"PPO training requires stable-baselines3 installation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75439969",
   "metadata": {},
   "source": [
    "## Section 8: Cross-Validation on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a9e70e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9f62ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2c0386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddd3c24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
